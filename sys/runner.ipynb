{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import gzip\n",
    "from datetime import datetime\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import re\n",
    "\n",
    "# your modules are imported here\n",
    "from indexing import Indexer, BasicInvertedIndex\n",
    "from document_preprocessor import RegexTokenizer, Doc2QueryAugmenter\n",
    "from ranker import Ranker, BM25, CrossEncoderScorer\n",
    "from vector_ranker import VectorRanker\n",
    "from l2r import L2RFeatureExtractor, L2RRanker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prefix = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_prefix + 'rec_cats.json', 'r') as f:\n",
    "    rec_cats = json.load(f)\n",
    "    top5cats = rec_cats['short']\n",
    "    cats = rec_cats['full']\n",
    "with open(data_prefix + 'doc_cat_info.json', 'r') as f:\n",
    "    doc_cat_info = json.load(f)\n",
    "    doc_cat_info = {int(k):v for k, v in doc_cat_info.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../interior_dic.json', 'r') as f:\n",
    "#     query_alts_rels = json.load(f)\n",
    "#     queries = list(query_alts_rels.keys())\n",
    "#     for query in queries:\n",
    "#         del query_alts_rels[query]['alt_qs'][5]\n",
    "#         for i in range(5):\n",
    "#             q = query_alts_rels[query]['alt_qs'][i]\n",
    "#             query_alts_rels[query]['alt_qs'][i] = q[3:]\n",
    "#         query_alts_rels[query]['scored_docs'] = [(l[0], l[1]) for l in query_alts_rels[query]['scored_docs']]\n",
    "#     train_queries = queries[:41]\n",
    "#     test_queries = queries[41:]\n",
    "# with open('../train_data.json', 'w') as f:\n",
    "#     train_data = {query: query_alts_rels[query] for query in train_queries}\n",
    "#     test_data = {query: query_alts_rels[query] for query in test_queries}\n",
    "#     json.dump(train_data, f, indent=2)\n",
    "# with open('../test_data.json', 'w') as f:\n",
    "#     json.dump(test_data, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug1 = Doc2QueryAugmenter()\n",
    "# aug2 = Doc2QueryAugmenter('doc2query/msmarco-t5-small-v1')\n",
    "# aug3 = Doc2QueryAugmenter('google/flan-t5-small')\n",
    "# prefix = \"Generate a query for the following text: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../final_data_with_categories.json', 'r') as f:\n",
    "#     lines = f.readlines()\n",
    "#     docs = []\n",
    "#     for line in tqdm(lines):\n",
    "#         doc = json.loads(line)\n",
    "#         doc['alt_qs'] = aug1.get_queries(doc['text'], 5)\n",
    "#         doc['dumb_qs_1'] = aug2.get_queries(doc['text'], 5)\n",
    "#         doc['dumb_qs_2'] = aug3.get_queries(doc['text'], 5, prefix)\n",
    "#         docs.append(doc)\n",
    "# with open('../data/doc_dataset.jsonl', 'a') as f:\n",
    "#     for doc in docs:\n",
    "#         f.write(json.dumps(doc) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/doc_dataset_old.jsonl', 'r') as f:\n",
    "#     line = f.readline()\n",
    "#     doc_inds = [m.start() for m in re.finditer('{\"docid\":', line)]\n",
    "#     docs = []\n",
    "#     for i in range(len(doc_inds)):\n",
    "#         start = doc_inds[i]\n",
    "#         end = len(line) if i == len(doc_inds) - 1 else doc_inds[i + 1]\n",
    "#         doc_text = line[start:end]\n",
    "#         doc = json.loads(doc_text)\n",
    "#         docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/doc_dataset.jsonl', 'a') as f:\n",
    "#     for doc in docs:\n",
    "#         f.write(json.dumps(doc) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stopwords collected 543'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_preproc = RegexTokenizer('\\\\w+')\n",
    "stopwords = set()\n",
    "with open(data_prefix + 'stopwords.txt', 'r', encoding='utf-8') as file:\n",
    "    for stopword in file:\n",
    "        stopwords.add(stopword.strip())\n",
    "f'Stopwords collected {len(stopwords)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_base_index = Indexer.create_index(data_prefix + 'doc_dataset.jsonl', doc_preproc, stopwords, 'text', 'alt_qs', data_prefix + 'doc_base_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_base_index = BasicInvertedIndex()\n",
    "doc_base_index.load(data_prefix + 'doc_base_index')\n",
    "doc_small_index = BasicInvertedIndex()\n",
    "doc_small_index.load(data_prefix + 'doc_small_index')\n",
    "doc_flan_index = BasicInvertedIndex()\n",
    "doc_flan_index.load(data_prefix + 'doc_flan_index')\n",
    "doc_index = BasicInvertedIndex()\n",
    "doc_index.load(data_prefix + 'doc_index')\n",
    "tit_index = BasicInvertedIndex()\n",
    "tit_index.load(data_prefix + 'title_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_small_index = Indexer.create_index(data_prefix + 'doc_dataset.jsonl', doc_preproc, stopwords, 'text', 'dumb_qs_1', data_prefix + 'doc_small_index')\n",
    "# doc_flan_index = Indexer.create_index(data_prefix + 'doc_dataset.jsonl', doc_preproc, stopwords, 'text', 'dumb_qs_2', data_prefix + 'doc_flan_index')\n",
    "# doc_index = Indexer.create_index(data_prefix + 'doc_dataset.jsonl', doc_preproc, stopwords, 'text', '', data_prefix + 'doc_index')\n",
    "# tit_index = Indexer.create_index(data_prefix + 'doc_dataset.jsonl', doc_preproc, stopwords, 'title', '', data_prefix + 'title_index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_runner(ranker, queries):\n",
    "    scores = [ranker.query(query) for query in queries]\n",
    "    final_scores = []\n",
    "    docids = [dic['docid'] for dic in scores[0]]\n",
    "    for doc in docids:\n",
    "        cum_score = 0\n",
    "        for score_l in scores:\n",
    "            for dic in score_l:\n",
    "                if dic['docid'] == doc:\n",
    "                    cum_score += dic['score']\n",
    "                    break\n",
    "        final_scores.append({'docid': doc, 'score': cum_score / len(queries)})\n",
    "    return final_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_prefix + 'raw_text.json', 'r') as f:\n",
    "    raw_text_dict = json.load(f)\n",
    "with open(data_prefix + 'base_raw_text.json', 'r') as f:\n",
    "    base_raw_text_dict = json.load(f)\n",
    "with open(data_prefix + 'small_raw_text.json', 'r') as f:\n",
    "    small_raw_text_dict = json.load(f)\n",
    "with open(data_prefix + 'flan_raw_text.json', 'r') as f:\n",
    "    flan_raw_text_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_scorer = CrossEncoderScorer(raw_text_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_extract = L2RFeatureExtractor(doc_index, tit_index, doc_cat_info, doc_preproc, stopword, set(top5cats), ce_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "docids = list(raw_text_dict.keys())\n",
    "model = SentenceTransformer(\"sentence-transformers/msmarco-MiniLM-L12-cos-v5\", device='cpu')\n",
    "texts = [raw_text_dict[docid] for docid in docids]\n",
    "embs = np.array([model.encode(text) for text in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(data_prefix + 'embs.npy', embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vranker = VectorRanker(\"sentence-transformers/msmarco-MiniLM-L12-cos-v5\", embs, docids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2r_ranker = L2RRanker(doc_index, tit_index, doc_preproc, stopword, vranker, feat_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import indexing\n",
    "reload(indexing)\n",
    "from indexing import Indexer\n",
    "import document_preprocessor\n",
    "reload(document_preprocessor)\n",
    "from document_preprocessor import RegexTokenizer, Doc2QueryAugmenter\n",
    "import l2r\n",
    "reload(l2r)\n",
    "from l2r import L2RFeatureExtractor, L2RRanker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance dict created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 2188.68it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 1303.20it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 5555.52it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 1293.96it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 1661.68it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 9102.62it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 6233.36it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 7567.94it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 1904.68it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 12186.37it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 6861.51it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 1910.22it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 2279.88it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 1732.07it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 100/100 [00:00<00:00, 12547.28it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 1502.95it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 6094.78it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 4122.33it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 10429.44it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 1709.80it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 2235.05it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 1855.84it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 5182.89it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 2381.86it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 2944.12it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 5673.50it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 1469.12it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 6496.15it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 1673.18it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 8444.68it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 9855.50it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 1510.39it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 2376.54it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 6890.82it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 5727.89it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 1205.46it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 5829.63it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 1653.38it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 2290.94it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 4783.76it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 2267.78it/s]\n",
      "preparing features:query cycle: 100%|██████████| 41/41 [00:01<00:00, 39.82it/s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "l2r_ranker.train(data_prefix + 'train_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only ('multilabel-indicator', 'continuous-multioutput', 'multiclass-multioutput') formats are supported. Got multiclass instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m true_relevance \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m])\n\u001b[1;32m      3\u001b[0m scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([\u001b[38;5;241m.1\u001b[39m, \u001b[38;5;241m.2\u001b[39m, \u001b[38;5;241m.3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m70\u001b[39m])\n\u001b[0;32m----> 4\u001b[0m \u001b[43mndcg_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_relevance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1820\u001b[0m, in \u001b[0;36mndcg_score\u001b[0;34m(y_true, y_score, k, sample_weight, ignore_ties)\u001b[0m\n\u001b[1;32m   1815\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1816\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1817\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing NDCG is only meaningful when there is more than 1 document. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1818\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_true\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1819\u001b[0m     )\n\u001b[0;32m-> 1820\u001b[0m \u001b[43m_check_dcg_target_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1821\u001b[0m gain \u001b[38;5;241m=\u001b[39m _ndcg_sample_scores(y_true, y_score, k\u001b[38;5;241m=\u001b[39mk, ignore_ties\u001b[38;5;241m=\u001b[39mignore_ties)\n\u001b[1;32m   1822\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39maverage(gain, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.5/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1518\u001b[0m, in \u001b[0;36m_check_dcg_target_type\u001b[0;34m(y_true)\u001b[0m\n\u001b[1;32m   1512\u001b[0m supported_fmt \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontinuous-multioutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1515\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass-multioutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1516\u001b[0m )\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m supported_fmt:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m formats are supported. Got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1520\u001b[0m             supported_fmt, y_type\n\u001b[1;32m   1521\u001b[0m         )\n\u001b[1;32m   1522\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Only ('multilabel-indicator', 'continuous-multioutput', 'multiclass-multioutput') formats are supported. Got multiclass instead"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "true_relevance = np.asarray([[10, 0, 0, 1, 5]])\n",
    "scores = np.asarray([[10, 0, 0, 0, 5]])\n",
    "ndcg_score(true_relevance, scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "si650",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
