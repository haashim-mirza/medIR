{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josh/miniconda3/envs/si650/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import gzip\n",
    "from datetime import datetime\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import re\n",
    "\n",
    "# your modules are imported here\n",
    "from indexing import Indexer, BasicInvertedIndex\n",
    "from document_preprocessor import RegexTokenizer, Doc2QueryAugmenter\n",
    "from ranker import Ranker, BM25, CrossEncoderScorer\n",
    "from vector_ranker import VectorRanker\n",
    "from l2r import L2RFeatureExtractor, L2RRanker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prefix = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_prefix + 'rec_cats.json', 'r') as f:\n",
    "    rec_cats = json.load(f)\n",
    "    top5cats = rec_cats['short']\n",
    "    cats = rec_cats['full']\n",
    "with open(data_prefix + 'doc_cat_info.json', 'r') as f:\n",
    "    doc_cat_info = json.load(f)\n",
    "    doc_cat_info = {int(k):v for k, v in doc_cat_info.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../interior_dic.json', 'r') as f:\n",
    "#     query_alts_rels = json.load(f)\n",
    "#     queries = list(query_alts_rels.keys())\n",
    "#     for query in queries:\n",
    "#         del query_alts_rels[query]['alt_qs'][5]\n",
    "#         for i in range(5):\n",
    "#             q = query_alts_rels[query]['alt_qs'][i]\n",
    "#             query_alts_rels[query]['alt_qs'][i] = q[3:]\n",
    "#         query_alts_rels[query]['scored_docs'] = [(l[0], l[1]) for l in query_alts_rels[query]['scored_docs']]\n",
    "#     train_queries = queries[:41]\n",
    "#     test_queries = queries[41:]\n",
    "# with open('../train_data.json', 'w') as f:\n",
    "#     train_data = {query: query_alts_rels[query] for query in train_queries}\n",
    "#     test_data = {query: query_alts_rels[query] for query in test_queries}\n",
    "#     json.dump(train_data, f, indent=2)\n",
    "# with open('../test_data.json', 'w') as f:\n",
    "#     json.dump(test_data, f, indent=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug1 = Doc2QueryAugmenter()\n",
    "# aug2 = Doc2QueryAugmenter('doc2query/msmarco-t5-small-v1')\n",
    "# aug3 = Doc2QueryAugmenter('google/flan-t5-small')\n",
    "# prefix = \"Generate a query for the following text: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../final_data_with_categories.json', 'r') as f:\n",
    "#     lines = f.readlines()\n",
    "#     docs = []\n",
    "#     for line in tqdm(lines):\n",
    "#         doc = json.loads(line)\n",
    "#         doc['alt_qs'] = aug1.get_queries(doc['text'], 5)\n",
    "#         doc['dumb_qs_1'] = aug2.get_queries(doc['text'], 5)\n",
    "#         doc['dumb_qs_2'] = aug3.get_queries(doc['text'], 5, prefix)\n",
    "#         docs.append(doc)\n",
    "# with open('../data/doc_dataset.jsonl', 'a') as f:\n",
    "#     for doc in docs:\n",
    "#         f.write(json.dumps(doc) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/doc_dataset_old.jsonl', 'r') as f:\n",
    "#     line = f.readline()\n",
    "#     doc_inds = [m.start() for m in re.finditer('{\"docid\":', line)]\n",
    "#     docs = []\n",
    "#     for i in range(len(doc_inds)):\n",
    "#         start = doc_inds[i]\n",
    "#         end = len(line) if i == len(doc_inds) - 1 else doc_inds[i + 1]\n",
    "#         doc_text = line[start:end]\n",
    "#         doc = json.loads(doc_text)\n",
    "#         docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/doc_dataset.jsonl', 'a') as f:\n",
    "#     for doc in docs:\n",
    "#         f.write(json.dumps(doc) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stopwords collected 543'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_preproc = RegexTokenizer('\\\\w+')\n",
    "stopwords = set()\n",
    "with open(data_prefix + 'stopwords.txt', 'r', encoding='utf-8') as file:\n",
    "    for stopword in file:\n",
    "        stopwords.add(stopword.strip())\n",
    "f'Stopwords collected {len(stopwords)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_base_index = Indexer.create_index(data_prefix + 'doc_dataset.jsonl', doc_preproc, stopwords, 'text', 'alt_qs', data_prefix + 'doc_base_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_base_index = BasicInvertedIndex()\n",
    "doc_base_index.load(data_prefix + 'doc_base_index')\n",
    "doc_small_index = BasicInvertedIndex()\n",
    "doc_small_index.load(data_prefix + 'doc_small_index')\n",
    "doc_flan_index = BasicInvertedIndex()\n",
    "doc_flan_index.load(data_prefix + 'doc_flan_index')\n",
    "doc_index = BasicInvertedIndex()\n",
    "doc_index.load(data_prefix + 'doc_index')\n",
    "tit_index = BasicInvertedIndex()\n",
    "tit_index.load(data_prefix + 'title_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_small_index = Indexer.create_index(data_prefix + 'doc_dataset.jsonl', doc_preproc, stopwords, 'text', 'dumb_qs_1', data_prefix + 'doc_small_index')\n",
    "# doc_flan_index = Indexer.create_index(data_prefix + 'doc_dataset.jsonl', doc_preproc, stopwords, 'text', 'dumb_qs_2', data_prefix + 'doc_flan_index')\n",
    "# doc_index = Indexer.create_index(data_prefix + 'doc_dataset.jsonl', doc_preproc, stopwords, 'text', '', data_prefix + 'doc_index')\n",
    "# tit_index = Indexer.create_index(data_prefix + 'doc_dataset.jsonl', doc_preproc, stopwords, 'title', '', data_prefix + 'title_index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_runner(ranker, queries):\n",
    "    scores = [ranker.query(query) for query in queries]\n",
    "    final_scores = []\n",
    "    docids = [dic['docid'] for dic in scores[0]]\n",
    "    for doc in docids:\n",
    "        cum_score = 0\n",
    "        for score_l in scores:\n",
    "            for dic in score_l:\n",
    "                if dic['docid'] == doc:\n",
    "                    cum_score += dic['score']\n",
    "                    break\n",
    "        final_scores.append({'docid': doc, 'score': cum_score / len(queries)})\n",
    "    return final_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_prefix + 'raw_text.json', 'r') as f:\n",
    "    raw_text_dict = json.load(f)\n",
    "with open(data_prefix + 'base_raw_text.json', 'r') as f:\n",
    "    base_raw_text_dict = json.load(f)\n",
    "with open(data_prefix + 'small_raw_text.json', 'r') as f:\n",
    "    small_raw_text_dict = json.load(f)\n",
    "with open(data_prefix + 'flan_raw_text.json', 'r') as f:\n",
    "    flan_raw_text_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_scorer = CrossEncoderScorer(raw_text_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_extract = L2RFeatureExtractor(doc_index, tit_index, doc_cat_info, doc_preproc, stopword, set(top5cats), ce_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "docids = list(raw_text_dict.keys())\n",
    "model = SentenceTransformer(\"sentence-transformers/msmarco-MiniLM-L12-cos-v5\", device='cpu')\n",
    "texts = [raw_text_dict[docid] for docid in docids]\n",
    "embs = np.array([model.encode(text) for text in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(data_prefix + 'embs.npy', embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vranker = VectorRanker(\"sentence-transformers/msmarco-MiniLM-L12-cos-v5\", embs, docids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2r_ranker = L2RRanker(doc_index, tit_index, doc_preproc, stopword, vranker, feat_extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import indexing\n",
    "reload(indexing)\n",
    "from indexing import Indexer\n",
    "import document_preprocessor\n",
    "reload(document_preprocessor)\n",
    "from document_preprocessor import RegexTokenizer, Doc2QueryAugmenter\n",
    "import l2r\n",
    "reload(l2r)\n",
    "from l2r import L2RFeatureExtractor, L2RRanker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relevance dict created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 2188.68it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 1303.20it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 5555.52it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 1293.96it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 1661.68it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 9102.62it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 6233.36it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 7567.94it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 1904.68it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 12186.37it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 6861.51it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 1910.22it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 2279.88it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 1732.07it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 100/100 [00:00<00:00, 12547.28it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 1502.95it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 6094.78it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 4122.33it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 10429.44it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 1709.80it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 2235.05it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 1855.84it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 5182.89it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 2381.86it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 2944.12it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 5673.50it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 1469.12it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 6496.15it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 1673.18it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 8444.68it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 9855.50it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 1510.39it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 2376.54it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 6890.82it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 5727.89it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 1205.46it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 5829.63it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 1653.38it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 2290.94it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 4783.76it/s]\n",
      "preparing features:doc cycle: 100%|██████████| 50/50 [00:00<00:00, 2267.78it/s]\n",
      "preparing features:query cycle: 100%|██████████| 41/41 [00:01<00:00, 39.82it/s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "l2r_ranker.train(data_prefix + 'train_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "si650",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
